{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YenH_9hJbFk1"
   },
   "source": [
    "# Práctica 2: Aprendiendo a utilizar spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pesar de existir multitud de librerías y herramientas para el procesamiento del lenguaje natural, debemos centrarnos en utilizar alguna en concreto (ya que nos vamos a hacer todo desde 0). Para ello, utilizaremos <a href=\"https://spacy.io/\">spaCy</a> (Descubrimiento de Aitor Angulo) que es una librería bastante expandida (con adaptaciones a varios idiomas) y lo que es mejor, con un **muy buen tutorial** sobre su uso. Para aprender a utilizarla, nos basaremos en <a href=\"https://course.spacy.io/es/\">estos mismos tutoriales</a> ya que están muy bien. El objetivo, por tanto, es tratar de hacer en este notebook todos los ejercicios y ejemplos de los 4 capítulos del curso."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instalación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbVhjPpzn6BM"
   },
   "source": [
    "Para instalarlo llevaremos a cabo los siguientes pasos:\n",
    "\n",
    "*   **Primero** lanzar la instalación de la librería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-09-23T00:11:56.338306Z",
     "iopub.status.busy": "2020-09-23T00:11:56.337617Z",
     "iopub.status.idle": "2020-09-23T00:12:03.150200Z",
     "shell.execute_reply": "2020-09-23T00:12:03.150753Z"
    },
    "id": "dzLKpmZICaWN"
   },
   "outputs": [],
   "source": [
    "# pip install -U pip setuptools wheel\n",
    "# pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Después** trataremos de evitar un error que ocurre actualmente al importar los modelos debido a la versión de una librería de la que depende. Para ello, instalaremos una versión estable de la dependencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install click==8.0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   **Por último**, importamos los modelos de embeddings en inglés y español:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Modelo de embedding preentrenado para el INGLÉS\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "# # Modelo de embedding preentrenado para el ESPAÑOL\n",
    "# python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Importa la librería spaCy\n",
    "import spacy\n",
    "# Importa el español\n",
    "from spacy.lang.es import Spanish\n",
    "# Importa las clases Matcher y PhraseMatcher\n",
    "from spacy.matcher import Matcher, PhraseMatcher\n",
    "# Importa las clases Doc y Span\n",
    "from spacy.tokens import Doc, Span\n",
    "# Importa la clase Language\n",
    "from spacy.language import Language\n",
    "# Importa las clases globales\n",
    "from spacy.tokens import Doc, Token, Span"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR0EdgrLCaWR"
   },
   "source": [
    "## Capítulo 1: Encontrando palabras, frases, nombres y conceptos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01: Introducción a spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El objeto nlp\n",
    "Contiene el pipeline de procesamiento incluye para tokenizar, etc., específicas para cada lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un objeto nlp vacío para procesar español\n",
    "nlp = spacy.blank(\"es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El objeto Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡\n",
      "Hola\n",
      "Mundo\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "# Creado al procesar un string de texto con el objeto nlp\n",
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Itera sobre los tokens en un Doc\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El objeto Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mundo\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Usa el índice del Doc para obtener un solo Token\n",
    "token = doc[2]\n",
    "\n",
    "# Obtén el texto del token a través del atributo .text\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### El objeto Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mundo!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"¡Hola Mundo!\")\n",
    "\n",
    "# Un slice de un Doc en un objeto Span\n",
    "span = doc[2:4]\n",
    "\n",
    "# Obtén el texto del span a través del atributo .text\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Atributos léxicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Índice:  [0, 1, 2, 3, 4]\n",
      "Texto:  ['Eso', 'cuesta', '€', '5', '.']\n",
      "is_alpha:  [True, True, False, False, False]\n",
      "is_punct:  [False, False, False, False, True]\n",
      "like_num:  [False, False, False, True, False]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Eso cuesta €5.\")\n",
    "print(\"Índice: \", [token.i for token in doc])\n",
    "print(\"Texto: \", [token.text for token in doc])\n",
    "\n",
    "print(\"is_alpha: \", [token.is_alpha for token in doc])\n",
    "print(\"is_punct: \", [token.is_punct for token in doc])\n",
    "print(\"like_num: \", [token.like_num for token in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02: Para empezar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1: Español\n",
    "\n",
    "* Usa spacy.blank para crear un objeto nlp vacío para procesar español (\"es\").\n",
    "* Crea un doc e imprime su texto en la pantalla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "# Crea el objeto nlp para procesar español\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"¿Cómo estás?\")\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2: Inglés\n",
    "\n",
    "* Usa spacy.blank para crear un objeto nlp vacío para procesar inglés (\"en\").\n",
    "* Crea un doc e imprime su texto en la pantalla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el objeto nlp para procesar inglés\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Procesa un texto (aquí dice \"Esta es una oración\" en inglés)\n",
    "doc = nlp(\"This is a sentence.\")\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 3: Alemán\n",
    "\n",
    "* Usa spacy.blank para crear un objeto nlp vacío para procesar alemán (\"de\").\n",
    "* Crea un doc e imprime su texto en la pantalla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el objeto nlp para procesar alemán\n",
    "nlp = spacy.blank(\"de\")\n",
    "\n",
    "# Procesa un texto (aquí dice \"Saludos cordiales!\" en alemán)\n",
    "doc = nlp(\"Liebe Grüße!\")\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03: Documentos, spans y tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cuando llamas nlp sobre un string, spaCy primero genera tokens del texto y crea un objeto del tipo documento (Doc). En este ejercicio aprenderás más sobre el Doc, así como de sus views Token y Span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1\n",
    "\n",
    "* Utiliza spacy.blank para crear el objeto nlp para procesar español.\n",
    "* Procesa el texto y genera un instance de un objeto Doc en la variable doc.\n",
    "* Selecciona el primer token del Doc e imprime su texto (text) en pantalla.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\"Me gustan las panteras negras y los leones.\")\n",
    "\n",
    "# Selecciona el primer token\n",
    "first_token = doc[0]\n",
    "\n",
    "# Imprime en pantalla el texto del token\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2\n",
    "\n",
    "* Utiliza spacy.blank para crear el objeto nlp para procesar español.\n",
    "* Procesa el texto y genera un instance de un objeto Doc en la variable doc.\n",
    "* Crea un slice de Doc para los tokens “panteras negras” y “panteras negras y los leones”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panteras negras\n",
      "panteras negras y los leones\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\"Me gustan las panteras negras y los leones.\")\n",
    "\n",
    "# Un slice del Doc para \"panteras negras\"\n",
    "panteras_negras = doc[3:5]\n",
    "print(panteras_negras.text)\n",
    "\n",
    "# Un slice del Doc para \"panteras negras y los leones\" (sin el \".\")\n",
    "panteras_negras_y_leones = doc[3:8]\n",
    "print(panteras_negras_y_leones.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04: Atributos léxicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo usarás los objetos Doc y Token de spaCy y los atributos léxicos para encontrar porcentajes en el texto. Estarás buscando dos tokens subsecuentes: un número y un símbolo de porcentaje.\n",
    "\n",
    "* Usa el atributo like_num del token para revisar si un token en el doc parece un número.\n",
    "* Toma el token que sigue al token actual en el documento. El índice del siguiente token en el doc es token.i + 1.\n",
    "* Revisa si el atributo text del siguiente token es un símbolo de porcentaje ”%“.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porcentaje encontrado: 60\n",
      "Porcentaje encontrado: 4\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(\n",
    "    \"En 1990, más del 60 % de las personas en Asia del Este se encontraban \"\n",
    "    \"en extrema pobreza. Ahora, menos del 4 % lo están.\"\n",
    ")\n",
    "\n",
    "# Itera sobre los tokens en el doc\n",
    "for token in doc:\n",
    "    # Revisa si el token parece un número\n",
    "    if token.like_num:\n",
    "        # Obtén el próximo token en el documento\n",
    "        next_token = doc[token.i+1]\n",
    "        # Revisa si el texto del siguiente token es igual a '%'\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Porcentaje encontrado:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05: Pipelines entrenados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué son los pipelines entrenados?\n",
    "\n",
    "* Le permiten a spaCy predecir atributos lingüísticos en contexto\n",
    "    * Etiquetado gramatical\n",
    "    * Dependencias sintácticas\n",
    "    * Entidades nombradas\n",
    "* Entrenados con textos de ejemplo anotados\n",
    "* Pueden ser actualizados con más ejemplos para afinar las predicciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paquetes de pipelines\n",
    "Un paquete con la etiqueta es_core_news_sm\n",
    "\n",
    "* Parámetros binarios\n",
    "* Vocabulario\n",
    "* Metadata (lenguaje, pipeline)\n",
    "* Archivo de configuración\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicción de etiquetas gramaticales.  \n",
    "Prediciendo dependencias sintácticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ella PRON nsubj\n",
      "comió VERB ROOT\n",
      "pizza NOUN obj\n"
     ]
    }
   ],
   "source": [
    "# Carga el pipeline pequeño de español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Ella comió pizza\")\n",
    "\n",
    "# Itera sobre los tokens\n",
    "for token in doc:\n",
    "    # Imprime en pantalla el texto y la etiqueta gramatical predicha\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediciendo entidades nombradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "más PER\n",
      "EE.UU. LOC\n",
      "iPhone MISC\n",
      "Galaxy Note 9 LOC\n"
     ]
    }
   ],
   "source": [
    "# Procesa un texto\n",
    "doc = nlp(\n",
    "    \"Apple es la marca que más satisfacción genera en EE.UU., \"\n",
    "    \"pero el iPhone, fue superado por el Galaxy Note 9\"\n",
    ")\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto y la etiqueta de la entidad\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tip: el método spacy.explain\n",
    "\n",
    "Obtén definiciones rápidas de las etiquetas más comunes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-GPE locations, mountain ranges, bodies of water\n",
      "noun, proper singular\n",
      "Miscellaneous entities, e.g. events, nationalities, products or works of art\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain(\"LOC\"))\n",
    "print(spacy.explain(\"NNP\"))\n",
    "print(spacy.explain(\"MISC\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06: Paquetes de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Qué no está incluido en un paquete con un modelo que puedes cargar a spaCy?\n",
    "\n",
    "* Un archivo de metadatos incluyendo el lenguaje, el pipeline y la licencia.\n",
    "\n",
    "* Parámetros binarios para hacer predicciones estadísticas.\n",
    "\n",
    "* __Los datos anotados con los que el modelo fue entrenado.__\n",
    "\n",
    "* Strings del vocabulario del pipeline y sus hashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07: Cargando modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los pipelines que estamos usando en este curso ya están pre-instalados. Para obtener más detalles sobre los pipelines entrenados de spaCy y cómo instalarlos en tu máquina revisa la documentación.\n",
    "\n",
    "* Usa spacy.load para cargar el modelo pequeño de español \"es_core_news_sm\".\n",
    "* Procesa el texto e imprime en pantalla el texto del documento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De acuerdo con la revista Fortune, Apple fue la empresa más admirada en el mundo entre 2008 y 2012.\n"
     ]
    }
   ],
   "source": [
    "# Carga el modelo \"es_core_news_sm\"\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Imprime en pantalla el texto del documento\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08: Predicciendo anotaciones lingüísticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora puedes probar uno de los paquetes de pipelines pre-entrenados de spaCy y ver sus predicciones en acción. ¡También puedes intentarlo con tu propio texto! Para averiguar lo que cada tag o label significa puedes llamar a spacy.explain en el loop. Por ejemplo, spacy.explain(\"PROPN\") o spacy.explain(\"GPE\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 1\n",
    "\n",
    "* Procesa el texto del objeto nlp y crea un doc.\n",
    "* Por cada token, imprime en pantalla su texto, su .pos_ (etiqueta gramatical) y su .dep_ (etiqueta de dependencia sintáctica).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "De          ADP       case      \n",
      "acuerdo     NOUN      fixed     \n",
      "con         ADP       fixed     \n",
      "la          DET       det       \n",
      "revista     NOUN      obl       \n",
      "Fortune     PROPN     appos     \n",
      ",           PUNCT     punct     \n",
      "Apple       PROPN     nsubj     \n",
      "fue         AUX       cop       \n",
      "la          DET       det       \n",
      "empresa     NOUN      ROOT      \n",
      "más         ADV       advmod    \n",
      "admirada    ADJ       amod      \n",
      "en          ADP       case      \n",
      "el          DET       det       \n",
      "mundo       NOUN      obl       \n",
      "entre       ADP       case      \n",
      "2008        NOUN      nmod      \n",
      "y           CCONJ     cc        \n",
      "2012        NOUN      conj      \n",
      ".           PUNCT     punct     \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # Obtén el texto del token, el part-of-speech tag y el dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    # Esto es solo por formato\n",
    "    print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parte 2\n",
    "\n",
    "* Procesa el texto y crea un objeto doc.\n",
    "* Itera sobre los doc.ents e imprime en pantalla el texto de la entidad y el atributo label_.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "revista Fortune ORG\n",
      "Apple ORG\n",
      "más admirada MISC\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"De acuerdo con la revista Fortune, Apple fue la empresa \"\n",
    "    \"más admirada en el mundo entre 2008 y 2012.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "# Itera sobre las entidades predichas\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto de la entidad y su etiqueta\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 Prediciendo entidades nombradas encontexto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos son estadísticos y no siempre son correctos. La corrección de sus predicciones depende de los datos de entrenamiento y del texto que estás procesando. Veamos un ejemplo.\n",
    "\n",
    "* Procesa el texto con el objeto doc.\n",
    "* Itera sobre las entidades e imprime en pantalla el texto de la entidad y la etiqueta.\n",
    "* Parece ser que el modelo no predijo “adidas zx”. Crea un span para esos tokens manualmente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olímpicos MISC\n",
      "Tokio 2020 MISC\n",
      "Entidad faltante: adidas zx\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "text = (\n",
    "    \"Los Olímpicos de Tokio 2020 son la inspiración para la nueva \"\n",
    "    \"colección de zapatillas adidas zx.\"\n",
    ")\n",
    "\n",
    "# Procesa el texto\n",
    "doc = nlp(text)\n",
    "\n",
    "\n",
    "# Itera sobre las entidades\n",
    "for ent in doc.ents:\n",
    "    # Imprime en pantalla el texto de la entidad y su etiqueta\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# Obtén el span para \"adidas zx\"\n",
    "adidas_zx = doc[14:16]\n",
    "\n",
    "# Imprime en pantalla el texto del span\n",
    "print(\"Entidad faltante:\", adidas_zx.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Encontrando patrones basados en reglas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Por qué no simplemente expresiones regulares?\n",
    "\n",
    "* Buscar patrones en objetos Doc, no solamente en strings\n",
    "* Buscar patrones en tokens y atributos de tokens\n",
    "* Usa las predicciones del modelo\n",
    "* Ejemplo: \"araña\" (verbo) vs. \"araña\" (sustantivo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buscar patrones\n",
    "\n",
    "* Listas de diccionarios, uno por token\n",
    "\n",
    "* Busca por textos exactos de tokens\n",
    "\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]\n",
    "\n",
    "* Busca por atributos léxicos\n",
    "\n",
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "* Busca por cualquier atributo del token\n",
    "\n",
    "[{\"LEMMA\": \"comprar\"}, {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando el Matcher\n",
    "\n",
    "\n",
    "* match_id: valor del hash del nombre del patrón\n",
    "* start: índice de inicio del span resultante\n",
    "* end: índice del final del span resultante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adidas zx\n"
     ]
    }
   ],
   "source": [
    "# Carga el modelo y crea un objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Inicializa el matcher con el vocabulario compartido\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Añade el patrón al matcher\n",
    "pattern = [{\"TEXT\": \"adidas\"}, {\"TEXT\": \"zx\"}]\n",
    "matcher.add(\"ADIDAS_PATTERN\", [pattern])\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Nuevos diseños de zapatillas en la colección adidas zx\")\n",
    "\n",
    "# Llama al matcher sobre el doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Llama al matcher sobre el doc\n",
    "doc = nlp(\"Nuevos diseños de zapatillas en la colección adidas zx\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matches:\n",
    "    # Obtén el span resultante\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrando por atributos léxicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"copa\"},\n",
    "    {\"LOWER\": \"mundial\"},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "\n",
    "doc = nlp(\"2014 Copa Mundial FIFA: Alemania ganó!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrando por otros atributos del token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"comer\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "doc = nlp(\"Camila prefería comer sushi. Pero ahora está comiendo pasta.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando operadores y cuantificadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [\n",
    "    {\"LEMMA\": \"comprar\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # opcional: encuentra 0 o 1 ocurrencias\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "\n",
    "doc = nlp(\"Me compré un smartphone. Ahora le estoy comprando aplicaciones.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11: Usando el Matcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultados: ['adidas zx']\n"
     ]
    }
   ],
   "source": [
    "# Importa el Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\n",
    "    \"Los Olímpicos de Tokio 2020 son la inspiración para la nueva \"\n",
    "    \"colección de zapatillas adidas zx.\"\n",
    ")\n",
    "\n",
    "# Inicializa el matcher con el vocabulario compartido\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Crea un patrón que encuentre dos tokens: \"adidas\" y \"zx\"\n",
    "pattern = [{\"TEXT\": \"adidas\"}, {\"TEXT\": \"zx\"}]\n",
    "\n",
    "# Añade el patrón al matcher\n",
    "matcher.add(\"ADIDAS_ZX_PATTERN\", [pattern])\n",
    "\n",
    "# Usa al matcher sobre el doc\n",
    "matches = matcher(doc)\n",
    "print(\"Resultados:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12: Escribiendo patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de resultados encontrados: 3\n",
      "Resultado encontrado: iOS 7\n",
      "Resultado encontrado: iOS 11\n",
      "Resultado encontrado: iOS 10\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "\n",
    "doc = nlp(\n",
    "    \"Después de hacer la actualización de iOS no notarás un rediseño \"\n",
    "    \"radical del sistema: no se compara con los cambios estéticos que \"\n",
    "    \"tuvimos con el iOS 7. La mayoría de las funcionalidades del iOS 11 \"\n",
    "    \"siguen iguales en el iOS 10.\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón para las versiones de iOS enteras\n",
    "# (\"iOS 7\", \"iOS 11\", \"iOS 10\")\n",
    "pattern = [{\"TEXT\": \"iOS\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"IOS_VERSION_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de resultados encontrados: 3\n",
      "Resultado encontrado: descargué Fortnite\n",
      "Resultado encontrado: descargando Minecraft\n",
      "Resultado encontrado: descargar Winzip\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"descargué Fortnite en mi computadora, pero no puedo abrir el juego. \"\n",
    "    \"Ayuda? Cuando estaba descargando Minecraft, conseguí la versión de Windows \"\n",
    "    \"donde tiene una carpeta '.zip' y usé el programa por defecto para \"\n",
    "    \"descomprimirlo…así que también tengo que descargar Winzip?\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón que encuentre una forma de \"descargar\" más un nombre propio\n",
    "pattern = [{\"LEMMA\": \"descargar\"}, {\"POS\": \"PROPN\"}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"DOWNLOAD_THINGS_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de resultados encontrados: 3\n",
      "Resultado encontrado: gigante tecnológico\n",
      "Resultado encontrado: lecciones virtuales\n",
      "Resultado encontrado: tecnologías avanzadas\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"El gigante tecnológico IBM está ofreciendo lecciones virtuales \"\n",
    "    \"sobre tecnologías avanzadas gratuitas en español.\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón para un sustantivo más uno o dos adjetivos\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"NOUN_ADJ_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de resultados encontrados: 3\n",
      "Resultado encontrado: gigante tecnológico\n",
      "Resultado encontrado: lecciones virtuales\n",
      "Resultado encontrado: tecnologías avanzadas\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "doc = nlp(\n",
    "    \"El gigante tecnológico IBM está ofreciendo lecciones virtuales \"\n",
    "    \"sobre tecnologías avanzadas gratuitas en español.\"\n",
    ")\n",
    "\n",
    "# Escribe un patrón para un sustantivo más uno o dos adjetivos\n",
    "pattern = [{\"POS\": \"NOUN\"}, {\"POS\": \"ADJ\"}, {\"POS\": \"ADJ\", \"OP\": \"?\"}]\n",
    "\n",
    "# Añade el patrón al matcher y usa el matcher sobre el documento\n",
    "matcher.add(\"NOUN_ADJ_PATTERN\", [pattern])\n",
    "matches = matcher(doc)\n",
    "print(\"Total de resultados encontrados:\", len(matches))\n",
    "\n",
    "# Itera sobre los resultados e imprime el texto del span\n",
    "for match_id, start, end in matches:\n",
    "    print(\"Resultado encontrado:\", doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capítulo 2: Análisis de datos a gran escala con spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01: Estructuras de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hash value: 32833993555699147\n",
      "string value: café\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Ines toma café\")\n",
    "print(\"hash value:\", nlp.vocab.strings[\"café\"])\n",
    "print(\"string value:\", nlp.vocab.strings[32833993555699147])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02: De strings a hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9565357104409163886\n",
      "gato\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Yo tengo un gato\")\n",
    "\n",
    "# Busca el hash para la palabra \"gato\"\n",
    "gato_hash = nlp.vocab.strings[\"gato\"]\n",
    "print(gato_hash)\n",
    "\n",
    "# Busca el gato_hash para obtener el string\n",
    "gato_string = nlp.vocab.strings[gato_hash]\n",
    "print(gato_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4317129024397789502\n",
      "PER\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"David Bowie tiene el label PER\")\n",
    "\n",
    "# Busca el hash para el label del string \"PER\"\n",
    "person_hash = nlp.vocab.strings[\"PER\"]\n",
    "print(person_hash)\n",
    "\n",
    "# Busca el person_hash para obtener el string\n",
    "person_string = nlp.vocab.strings[person_hash]\n",
    "print(person_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04: Estructuras de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un objeto nlp\n",
    "import spacy\n",
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Importa la clase Doc\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "\n",
    "# Crea un doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las palabras y espacios que usaremos para crear el doc\n",
    "words = [\"¡\", \"Hola\", \"Mundo\", \"!\"]\n",
    "spaces = [False, True, False, False]\n",
    "\n",
    "# Crea un doc manualmente\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Crea un span manualmente\n",
    "span = Span(doc, 1, 3)\n",
    "\n",
    "# Crea un span con un label\n",
    "span_with_label = Span(doc, 1, 3, label=\"SALUDO\")\n",
    "\n",
    "# Añade el span a los doc.ents\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05: Creando un Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy es divertido!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# El texto deseado: \"spaCy es divertido!\"\n",
    "words = [\"spaCy\", \"es\", \"divertido\", \"!\"]\n",
    "spaces = [True, True, False, False]\n",
    "\n",
    "# Crea un Doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Vamos, empieza!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# El texto deseado: \"¡Vamos, empieza!\"\n",
    "words = [\"¡\", \"Vamos\", \",\", \"empieza\", \"!\"]\n",
    "spaces = [False, False, True, False, False]\n",
    "\n",
    "# Crea un Doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡¿En serio?!\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# El texto deseado: \"¡¿En serio?!\"\n",
    "words = [\"¡\", \"¿\", \"En\", \"serio\", \"?\", \"!\"]\n",
    "spaces = [False, False, True, False, False, False]\n",
    "\n",
    "# Crea un Doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06: Docs, spans y entidades desde cero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Me gusta David Bowie\n",
      "David Bowie Person\n",
      "[('David Bowie', 'Person')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "words = [\"Me\", \"gusta\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# Crea un doc a partir de las palabras y los espacios\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(doc.text)\n",
    "\n",
    "# Crea un span para \"David Bowie\" a partir del doc y asígnalo al label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"Person\")\n",
    "print(span.text, span.label_)\n",
    "\n",
    "# Añade el span a las entidades del doc\n",
    "doc.ents = [span]\n",
    "\n",
    "# Imprime en pantalla el texto y los labels de las entidades\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07: Buenas prácticas de las estructuras de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontré un nombre propio antes de un verbo: Berlín\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Por Berlín fluye el río Esprea.\")\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\"Por Berlín fluye el río Esprea.\")\n",
    "\n",
    "# Itera sobre los tokens\n",
    "for token in doc:\n",
    "    # Revisa si el token actual es un nombre propio\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        # Revisa si el siguiente token es un verbo\n",
    "        if doc[token.i + 1].pos_ == \"VERB\":\n",
    "            print(\"Encontré un nombre propio antes de un verbo:\", token.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08: Word vectors y similitud semántica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga uno de los modelos más grandes que contiene vectores\n",
    "nlp = spacy.load(\"es_core_news_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9513663710080219\n"
     ]
    }
   ],
   "source": [
    "# Compara dos documentos\n",
    "doc1 = nlp(\"Me gusta la comida rápida\")\n",
    "doc2 = nlp(\"Me gusta la pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6704208850860596\n"
     ]
    }
   ],
   "source": [
    "# Compara dos tokens\n",
    "doc = nlp(\"Me gustan la pizza y las hamburguesas\")\n",
    "token1 = doc[3]\n",
    "token2 = doc[6]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13637736545255463\n"
     ]
    }
   ],
   "source": [
    "# Compara un documento con un token\n",
    "doc = nlp(\"Me gusta la pizza\")\n",
    "token = nlp(\"jabón\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2730798551780409\n"
     ]
    }
   ],
   "source": [
    "# Compara un span con un documento\n",
    "span = nlp(\"Me gustan los perros calientes\")[3:5]\n",
    "doc = nlp(\"McDonalds vende hamburguesas\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.58136    0.037496   0.66934    2.7966    -0.023352   0.39145\n",
      "  0.55106    0.2597     2.6257     3.1932    -0.49274    0.084971\n",
      "  0.083045  -1.1788    -0.11184    0.052108  -0.563      0.2155\n",
      " -1.5244    -1.9768    -1.6693    -0.85393    0.89011   -0.99332\n",
      "  1.7136    -1.7498    -1.5536     0.44981    0.76886    1.298\n",
      "  0.094683  -0.07842    1.1843    -1.5305    -0.44663    1.3727\n",
      "  1.2239    -1.4967     0.75916    0.70923    1.4964     0.56073\n",
      " -1.6018    -0.91338   -2.0583     1.1208    -0.86255    0.76231\n",
      "  0.60926   -1.0933    -2.0223    -1.2327     0.24917    0.95122\n",
      " -1.0975    -0.83044   -1.4911    -0.79706   -0.23831    0.10205\n",
      " -0.44532    1.0172     1.2452    -2.0414    -0.39335    1.149\n",
      " -0.0094314  1.5695    -2.2982     1.2705     0.59918    0.95636\n",
      "  0.20392    0.35687    1.6167    -0.80719   -0.52339    0.68923\n",
      " -0.3944    -3.0173     1.0634    -3.4248    -0.29587   -1.2832\n",
      " -1.6083     0.74691   -0.11828    1.4702     0.16136   -1.4201\n",
      "  0.70638   -0.072624  -1.4466     0.72344    0.078114   1.2596\n",
      " -1.8129    -0.65629   -0.80641   -1.1449    -2.545      0.85491\n",
      "  0.54189    1.8337    -0.20536    0.12252   -2.2903    -0.1698\n",
      "  0.51621   -2.0066     0.89327   -0.91274   -0.97247    1.1017\n",
      "  2.8891    -0.81364    0.022315   1.5598    -1.8567    -1.3784\n",
      "  1.2099     0.5902    -1.3276     0.24976    0.94643    0.31263\n",
      " -1.7238    -1.5393     1.7214    -0.83761    1.0829     1.0269\n",
      " -1.2177    -0.079954   0.4957     0.7978    -1.3404     1.5048\n",
      "  0.42446    2.1174    -0.72877    0.57396    0.59383   -0.24617\n",
      "  1.004      1.1322    -0.6847     0.63397   -1.3758     1.126\n",
      "  0.087296   0.17559    0.66427   -0.39574    0.36193    2.6496\n",
      "  1.4788    -0.20837   -0.13546   -1.6081     0.76137    0.40225\n",
      " -2.3385    -0.37009    1.5414    -0.37543   -0.74023    0.37674\n",
      " -1.5544    -0.43297   -1.5797     1.0251     1.283     -1.8788\n",
      " -0.084524   0.76776   -1.4217     1.289     -0.063222   0.56392\n",
      " -0.43796   -0.52452   -0.35984    0.91657    1.5457    -1.3059\n",
      "  1.0045    -1.6402     0.74614   -0.88665   -0.70848    0.43385\n",
      " -0.61166   -0.6628     0.40492   -0.81584   -0.10816    1.4434\n",
      " -1.3407    -0.12776   -0.35255   -1.6831    -2.9178    -1.2738\n",
      "  0.95345   -0.20384    0.70905   -1.1009    -0.98886   -0.78085\n",
      " -0.68736    0.68719    0.39532    0.7868     0.9388     0.26213\n",
      "  0.51041    1.2448     0.39292    0.56005    0.16127   -0.26196\n",
      " -0.52357   -0.82289    1.0116    -0.036065   2.4301     0.44541\n",
      "  0.30908   -0.5454    -0.66264   -1.1558     0.15025    0.60381\n",
      " -1.9326     0.55429   -0.80461    0.525     -0.37794    1.2012\n",
      " -0.41166   -0.68876   -1.6461    -1.3226    -0.057329   1.8578\n",
      "  1.0736    -0.2627    -1.0381     0.14438   -1.5073     0.6385\n",
      "  0.14405    1.7461     1.14       0.1884     0.25404    0.3859\n",
      " -0.55736   -0.54779    1.6302    -0.054984   1.1899    -0.44067\n",
      " -1.0749    -0.66979   -1.4983    -0.40309    1.7218     0.64941\n",
      " -1.0417    -1.2201     0.025452  -1.3249     0.8728    -0.88773\n",
      "  0.36258    0.43347    1.3337    -0.68388    1.0006    -1.3476\n",
      " -2.2166    -0.44404   -1.2076     1.2248     1.0834     0.10092\n",
      " -0.22438   -0.3598     1.3011     1.2147     0.2494    -0.39024\n",
      " -0.44607    0.41497    0.6931    -0.5522     0.51031    0.058363 ]\n"
     ]
    }
   ],
   "source": [
    "# Carga uno de los modelos más grandes que contiene vectores\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc = nlp(\"Tengo una manzana\")\n",
    "# Accede al vector a través del atributo token.vector\n",
    "print(doc[2].vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9709654355279296\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"Me gustan los gatos\")\n",
    "doc2 = nlp(\"Me desagradan los gatos\")\n",
    "\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09: Inspeccinando los word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.9996e+00 -4.2573e-02  3.9818e-01  6.2750e-01  1.8673e+00 -5.2907e-01\n",
      "  2.1402e+00 -1.1613e+00  2.4252e-01  3.1764e+00  1.1129e+00 -1.9224e+00\n",
      " -5.7645e-02 -9.7982e-01  7.3552e-01  1.3192e+00  3.8898e-01  2.8240e+00\n",
      " -4.5733e-01  7.5970e-01  9.6714e-01 -1.1632e+00  1.0282e+00  3.9119e-01\n",
      " -3.5303e-02 -2.2249e+00  4.9892e-01 -1.7602e-01  8.9286e-01 -3.7965e-01\n",
      " -9.6017e-01  2.3329e+00 -6.1804e-02 -1.8836e+00 -1.6884e+00  1.0575e+00\n",
      " -7.3111e-03 -5.2787e-01  2.8302e+00 -1.6037e+00  2.4177e+00  1.5121e+00\n",
      "  6.7258e-01  1.1733e+00 -3.0453e+00  1.7522e+00 -4.9296e-01  2.4113e+00\n",
      "  1.0292e-01 -1.0871e+00  8.4645e-02 -5.3616e-01  2.4636e+00 -5.5531e-02\n",
      " -3.4077e+00 -2.6240e+00 -1.9075e-01  9.5506e-01  1.1819e+00  1.0649e+00\n",
      " -3.7705e+00  1.5252e+00 -2.1865e-01  1.4864e+00  1.7795e+00 -1.0925e+00\n",
      "  3.6124e-01 -1.7459e+00 -9.4558e-01  1.3766e-01  3.6593e+00  5.3245e-02\n",
      "  8.0833e-01  1.2607e+00  5.0965e-01 -3.0536e+00  6.6521e-01  1.6600e+00\n",
      "  5.1025e-01 -3.5286e+00  3.9231e-01 -3.0514e+00 -1.6318e+00 -5.5688e-01\n",
      " -2.9024e-01  5.7919e-01 -2.3618e+00  4.5326e-01 -1.5070e+00 -2.4876e-01\n",
      " -4.8697e-01  3.5460e-01  2.5023e-01  1.8110e+00  2.6347e+00  1.6482e+00\n",
      "  2.3309e-02 -1.7719e-01 -4.6191e+00 -6.5884e-01 -9.3746e-01  3.4030e+00\n",
      "  9.1304e-01  8.3850e-01  6.2015e-03  6.3440e-01 -2.2513e+00 -1.2498e+00\n",
      " -1.0183e+00  3.2721e-01  1.1560e+00 -2.5242e+00  1.7578e+00 -1.1051e+00\n",
      "  3.2901e+00 -2.1352e+00  5.3716e-01  4.5665e-01 -4.2371e-01 -9.0029e-02\n",
      "  6.5253e-01  2.5169e-01 -1.9402e+00  9.4248e-01 -1.8796e+00  4.2780e-02\n",
      " -3.2023e+00 -1.1647e+00  3.4456e+00  1.2335e+00 -7.9028e-01 -9.6117e-01\n",
      "  1.8470e+00  3.2246e+00  4.5099e-01  2.9149e-01 -6.2173e-01  4.1801e+00\n",
      "  9.7079e-01  2.0030e+00  1.6257e+00  2.5173e+00  1.7687e+00 -4.5209e-01\n",
      " -5.0393e-01  2.1320e+00 -9.7164e-01  1.1228e+00 -2.5499e+00  2.1094e+00\n",
      " -9.6575e-01  1.7154e+00  1.5136e+00  5.7725e-01  1.8468e+00  2.3616e+00\n",
      "  7.1222e-01  1.0796e+00  1.2445e+00 -2.6219e-01 -1.4237e-01  5.5114e-02\n",
      "  8.7271e-02 -1.7528e+00  3.1865e-01  7.9753e-02 -9.4706e-01  7.8647e-01\n",
      " -1.1306e-01 -1.9503e+00 -1.7766e+00 -1.0439e+00 -5.9059e-01 -3.1171e+00\n",
      " -8.5423e-01 -3.2388e-01  5.3954e-03 -1.0055e+00 -1.9603e+00 -1.0488e+00\n",
      "  1.6217e-02  7.2404e-02  8.1778e-01 -8.0956e-01  1.9893e+00 -2.1620e+00\n",
      "  9.6002e-01 -1.7981e+00 -3.3132e+00 -1.7572e+00  1.7231e-01  6.5694e-01\n",
      " -2.2566e+00  1.0523e+00  3.3480e-01 -1.1446e+00  5.4747e-01  1.8586e+00\n",
      " -2.1700e+00  1.0726e+00 -2.4079e+00  2.1341e-01 -1.8260e+00 -3.0536e-01\n",
      " -6.1021e-01 -1.5839e+00 -5.1896e-01  4.1283e+00  6.1982e-01  2.8812e-01\n",
      " -1.4007e+00 -9.7978e-01 -1.3724e+00 -1.3412e+00  1.8041e+00  1.0486e+00\n",
      " -4.4553e-01 -9.1084e-01 -7.6140e-01  2.2846e-01 -8.7839e-01  1.0705e+00\n",
      " -3.7184e+00  1.3568e+00  9.7216e-01  8.3766e-01 -1.5205e+00  9.9707e-01\n",
      "  7.1154e-01  3.9266e-01 -1.2005e+00 -1.4891e+00 -1.0855e+00  2.3482e+00\n",
      "  8.1906e-02  1.8286e-01  3.0204e+00  1.5883e+00 -9.4321e-01 -5.6585e-01\n",
      "  1.3465e-01 -7.5893e-02 -1.6556e+00 -6.8319e-02 -1.5057e-01  2.9611e-03\n",
      "  2.9132e-01  5.9993e-01 -6.5891e-01  1.7241e+00 -3.6649e+00  2.8233e+00\n",
      "  2.1320e-01 -2.8264e-01  1.5699e+00 -2.2614e-01  2.6608e+00 -1.2694e+00\n",
      " -8.1057e-01  2.7461e+00  1.3143e+00 -2.2874e+00  1.8970e+00 -1.0550e+00\n",
      "  6.7657e-01  2.7474e+00 -1.5905e+00  1.4040e+00  1.4359e+00 -2.3371e+00\n",
      "  1.1244e+00 -3.0514e+00  3.1420e+00 -6.6356e-01  1.5340e+00  2.1035e+00\n",
      "  1.9526e-01  2.3852e+00  2.0242e+00  1.1473e+00 -1.2414e+00 -2.6156e+00\n",
      " -3.5988e-01  3.1922e-01  3.3445e-01 -1.2538e+00  2.8046e+00 -1.2310e+00\n",
      "  1.8978e+00 -2.0322e+00  4.4676e-01  1.7870e+00  1.6610e+00 -7.3121e-01\n",
      " -2.8075e-01 -1.4680e+00  1.2969e+00  1.4855e+00  1.6161e+00 -1.8140e+00]\n"
     ]
    }
   ],
   "source": [
    "# Carga el modelo es_core_news_md\n",
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Hoy hice pan de banano\")\n",
    "\n",
    "# Obtén el vector para el token \"banano\"\n",
    "banano_vector = doc[4].vector\n",
    "print(banano_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Prediciendo similitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25600454111484433\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc1 = nlp(\"Es un cálido día de verano\")\n",
    "doc2 = nlp(\"Hay sol afuera\")\n",
    "\n",
    "# Obtén la similitud entre el doc1 y el doc2\n",
    "similarity = doc1.similarity(doc2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009235156700015068\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc = nlp(\"TV y libros\")\n",
    "token1, token2 = doc[0], doc[2]\n",
    "\n",
    "# Obtén la similitud entre los tokens \"TV\" y \"libros\"\n",
    "similarity = token1.similarity(token2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6768035888671875\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_md\")\n",
    "\n",
    "doc = nlp(\n",
    "    \"Estuvimos en un restaurante genial. Luego, fuimos a un bar muy divertido.\"\n",
    ")\n",
    "\n",
    "# Crea los spans para \"restaurante genial\" y \"bar muy divertido\"\n",
    "span1 = doc[3:5]\n",
    "span2 = doc[11:14]\n",
    "\n",
    "# Obtén la similitud entre los dos spans\n",
    "similarity = span1.similarity(span2)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11: Combinando modelos y reglas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa con el vocabulario compartido\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Los patrones son listas de diccionarios que describen los tokens\n",
    "pattern = [{\"LEMMA\": \"comer\", \"POS\": \"VERB\"}, {\"LOWER\": \"pizza\"}]\n",
    "matcher.add(\"PIZZA\", [pattern])\n",
    "\n",
    "# Los operadores pueden especificar qué tan seguido puede\n",
    "# ser buscado un token\n",
    "pattern = [{\"TEXT\": \"muy\", \"OP\": \"+\"}, {\"TEXT\": \"feliz\"}]\n",
    "matcher.add(\"MUY_FELIZ\", [pattern])\n",
    "\n",
    "# Llamar al matcher sobre un doc devuelve una lista de\n",
    "# tuples con (match_id, inicio, final)\n",
    "doc = nlp(\"Me gusta comer pizza y estoy muy muy feliz\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span encontrado: labrador dorado\n",
      "Token raíz: labrador\n",
      "Token raíz cabeza: Tengo\n",
      "Token anterior: un DET\n"
     ]
    }
   ],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERRO\", [[{\"LOWER\": \"labrador\"}, {\"LOWER\": \"dorado\"}]])\n",
    "doc = nlp(\"Tengo un labrador dorado\")\n",
    "\n",
    "for match_id, start, end in matcher(doc):\n",
    "    span = doc[start:end]\n",
    "    print(\"span encontrado:\", span.text)\n",
    "    # Obtén el token raíz del span y el token raíz cabeza (head)\n",
    "    print(\"Token raíz:\", span.root.text)\n",
    "    print(\"Token raíz cabeza:\", span.root.head.text)\n",
    "    # Obtén el token anterior y su POS tag\n",
    "    print(\"Token anterior:\", doc[start - 1].text, doc[start - 1].pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span resultante: labrador dorado\n"
     ]
    }
   ],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"labrador dorado\")\n",
    "matcher.add(\"PERRO\", [pattern])\n",
    "doc = nlp(\"Tengo un labrador dorado\")\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Obtén el span resultante\n",
    "    span = doc[start:end]\n",
    "    print(\"span resultante:\", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13: Debugging de patrones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATTERN1 40 años\n",
      "PATTERN1 40 aniversario\n",
      "PATTERN2 Pac-Man Live\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "doc = nlp(\n",
    "    \"Cuando Pac-Man debutó en Tokio, en 1980, nadie podría haber predicho \"\n",
    "    \"que se convertiría en el videojuego más exitoso de todos los tiempos. \"\n",
    "    \"Hoy, 40 años después, aun sigue sorprendiendo. Su desarrolladora, \"\n",
    "    \"Bandai Namco, ha anunciado novedades en el marco del aniversario del \"\n",
    "    \"juego. La celebración del 40 aniversario de Pac-man en 2020 incluirá \"\n",
    "    \"el debut de una nueva canción temática, compuesta por el famoso artista \"\n",
    "    \"japonés de Techno Ken Ishii. Además de estas novedades, Bandai Namco \"\n",
    "    \"publicará nuevas versiones del videojuego. La primera será Pac-Man Live \"\n",
    "    \"Studio, en Twitch, en colaboración con Amazon Games.\"\n",
    ")\n",
    "\n",
    "# Crea los patrones\n",
    "pattern1 = [{\"LIKE_NUM\": True}, {\"POS\": \"NOUN\"}]\n",
    "pattern2 = [{\"LOWER\": \"pac-man\"}, {\"IS_TITLE\": True}]\n",
    "\n",
    "# Inicializa el Matcher y añade los patrones\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Imprime en pantalla el nombre en string del patrón y\n",
    "    # el texto del span encontrado\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14: Encontrando frases eficientemente \"phrase matching\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'exercises/es/countries.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11740/3249436008.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"exercises/es/countries.json\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mCOUNTRIES\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblank\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"es\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m doc = nlp(\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'exercises/es/countries.json'"
     ]
    }
   ],
   "source": [
    "with open(\"exercises/es/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"es\")\n",
    "doc = nlp(\n",
    "    \"La Unión Europea fue fundada por seis países de Europa occidental \"\n",
    "    \"(Francia, Alemania, Italia, Bélgica, Países Bajos, y Luxemburgo) y \"\n",
    "    \"se amplió en seis ocasiones.\"\n",
    ")\n",
    "\n",
    "# Importa el PhraseMatcher e inicialízalo\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "# Crea objetos Doc patrón y añádelos al matcher\n",
    "# Esta es una versión más rápida de: [nlp(country) for country in COUNTRIES]\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Llama al matcher sobre el documento de prueba e imprime el \n",
    "# resultado en pantalla\n",
    "matches = matcher(doc)\n",
    "print([doc[start:end] for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15: Extrayendo países y relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"exercises/es/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "with open(\"exercises/es/country_text.txt\", encoding=\"utf8\") as f:\n",
    "    TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", None, *patterns)\n",
    "\n",
    "# Crea un doc y restablece las entidades existentes\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Itera sobre los resultados\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Crea un Span con la etiqueta para \"LOC\"\n",
    "    span = Span(doc, start, end, label=\"LOC\")\n",
    "\n",
    "    # Sobrescribe el doc.ents y añade el span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Obtén el token cabeza de la raíz del span\n",
    "    span_root_head = span.root.head\n",
    "    # Imprime en pantalla el texto del token cabeza de\n",
    "    # la raíz del span y el texto del span\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Imprime en pantalla las entidades del documento\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"LOC\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capítulo 3: Pipelines de procesamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03: Inspeccionando el pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec object at 0x00000289127961C0>), ('morphologizer', <spacy.pipeline.morphologizer.Morphologizer object at 0x0000028912796E80>), ('parser', <spacy.pipeline.dep_parser.DependencyParser object at 0x00000289127473C0>), ('attribute_ruler', <spacy.pipeline.attributeruler.AttributeRuler object at 0x00000288B64C2E80>), ('lemmatizer', <spacy.lang.es.lemmatizer.SpanishLemmatizer object at 0x00000288F49BE140>), ('ner', <spacy.pipeline.ner.EntityRecognizer object at 0x0000028912747510>)]\n"
     ]
    }
   ],
   "source": [
    "# Carga el modelo es_core_news_sm\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Imprime en pantalla los nombres de los componentes del pipeline\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Imprime en pantalla el pipeline entero de tuples (name, component)\n",
    "print(nlp.pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04: Componentes personalizados del pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "# Crea el objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Define un componente personalizado\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Imprime la longitud del doc en pantalla\n",
    "    print(\"longitud del Doc:\", len(doc))\n",
    "    # Devuelve el objeto doc\n",
    "    return doc\n",
    "\n",
    "# Añade el componente al primer lugar del pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Imprime los nombres de los componentes del pipeline\n",
    "print(\"Pipeline:\", nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "longitud del Doc: 4\n"
     ]
    }
   ],
   "source": [
    "# Crea el objeto nlp\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Define un componente personalizado\n",
    "@Language.component(\"custom_component\")\n",
    "def custom_component_function(doc):\n",
    "    # Imprime la longitud del doc en pantalla\n",
    "    print(\"longitud del Doc:\", len(doc))\n",
    "    # Devuelve el objeto doc\n",
    "    return doc\n",
    "\n",
    "# Añade el componente al primer lugar del pipeline\n",
    "nlp.add_pipe(\"custom_component\", first=True)\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"¡Hola Mundo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06: Componentes simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['length_component', 'tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n",
      "Este documento tiene 5 tokens.\n"
     ]
    }
   ],
   "source": [
    "# Define el componente personalizado\n",
    "@Language.component(\"length_component\")\n",
    "def length_component_function(doc):\n",
    "    # Obtén la longitud del doc\n",
    "    doc_length = len(doc)\n",
    "    print(f\"Este documento tiene {doc_length} tokens.\")\n",
    "    # Devuelve el doc\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Carga el modelo pequeño de español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Añade el componente en el primer lugar del pipeline e imprime\n",
    "# los nombres de los pipes en pantalla\n",
    "nlp.add_pipe(\"length_component\", first=True)\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa un texto\n",
    "doc = nlp(\"Esto es una frase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07: Componentes complejos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "patrones_de_animales: [labrador dorado, gato, tortuga, oso de anteojos]\n",
      "['tok2vec', 'morphologizer', 'parser', 'attribute_ruler', 'lemmatizer', 'ner', 'animal_component']\n",
      "[('tortuga', 'ANIMAL'), ('oso de anteojos', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "animals = [\"labrador dorado\", \"gato\", \"tortuga\", \"oso de anteojos\"]\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "print(\"patrones_de_animales:\", animal_patterns)\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define el componente personalizado\n",
    "@Language.component(\"animal_component\")\n",
    "def animal_component_function(doc):\n",
    "    # Aplica el matcher al doc\n",
    "    matches = matcher(doc)\n",
    "    # Crea un Span para cada resultado y asígnales el label \"ANIMAL\"\n",
    "    spans = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    # Sobrescribe los doc.ents con los spans resultantes\n",
    "    doc.ents = spans\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Añade el componente al pipeline después del componente \"ner\"\n",
    "nlp.add_pipe(\"animal_component\", after=\"ner\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el texto y el label\n",
    "# de los doc.ents\n",
    "doc = nlp(\"Hoy vimos una tortuga y un oso de anteojos en nuestra caminata\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08: Extensión de atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade extensiones para el Doc, Token y Span\n",
    "Doc.set_extension(\"title\", default=None, force=True)\n",
    "Token.set_extension(\"is_color\", default=False, force=True)\n",
    "Span.set_extension(\"has_color\", default=False, force=True)\n",
    "\n",
    "doc._.title = \"Mi documento\"\n",
    "token._.is_color = True\n",
    "span._.has_color = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Añade una extensión en el Token con un valor por defecto\n",
    "Token.set_extension(\"is_color\", default=False, force=True)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "\n",
    "# Sobrescribe el valor de la extensión de atributo\n",
    "doc[3]._.is_color = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - azul\n"
     ]
    }
   ],
   "source": [
    "# Define la función getter\n",
    "def get_is_color(token):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return token.text in colors\n",
    "\n",
    "# Añade una extensión en el Token con getter\n",
    "Token.set_extension(\"is_color\", getter=get_is_color, force=True)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[3]._.is_color, \"-\", doc[3].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - cielo es azul\n",
      "False - El cielo\n"
     ]
    }
   ],
   "source": [
    "# Define la función getter\n",
    "def get_has_color(span):\n",
    "    colors = [\"rojo\", \"amarillo\", \"azul\"]\n",
    "    return any(token.text in colors for token in span)\n",
    "\n",
    "# Añade una extensión en el Span con getter\n",
    "Span.set_extension(\"has_color\", getter=get_has_color, force=True)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc[1:4]._.has_color, \"-\", doc[1:4].text)\n",
    "print(doc[0:2]._.has_color, \"-\", doc[0:2].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True - azul\n",
      "False - nube\n"
     ]
    }
   ],
   "source": [
    "# Define un método con argumentos\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Añade una extensión en el Doc con el método\n",
    "Doc.set_extension(\"has_token\", method=has_token, force=True)\n",
    "\n",
    "doc = nlp(\"El cielo es azul.\")\n",
    "print(doc._.has_token(\"azul\"), \"- azul\")\n",
    "print(doc._.has_token(\"nube\"), \"- nube\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09: Añadiendo extensiones de atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 1\n",
    "\n",
    "* Usa Token.set_extension para registrar \"is_country\" (por defecto False).\n",
    "* Actualízalo para \"España\" e imprímelo en pantalla para todos los tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Vivo', False), ('en', False), ('España', True), ('.', False)]\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.blank(\"es\")\n",
    "\n",
    "# Registra la extensión de atributo del Token, \"is_country\",\n",
    "# con el valor por defecto False \n",
    "Token.set_extension(\"is_country\", default=False, force=True)\n",
    "\n",
    "# Procesa el texto y pon True para el atributo \"is_country\"\n",
    "# para el token \"España\"\n",
    "doc = nlp(\"Vivo en España.\")\n",
    "doc[2]._.is_country = True\n",
    "\n",
    "# Imprime en pantalla el texto del token y el atributo \"is_country\"\n",
    "# para todos los tokens\n",
    "print([(token.text, token._.is_country) for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Paso 2\n",
    "\n",
    "* Usa Token.set_extension para registrar \"reversed\" (función getter get_reversed).  \n",
    "* Imprime en pantalla su valor por cada token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invertido: sadoT\n",
      "invertido: sal\n",
      "invertido: senoicazilareneg\n",
      "invertido: nos\n",
      "invertido: saslaf\n",
      "invertido: ,\n",
      "invertido: odneyulcni\n",
      "invertido: atse\n",
      "invertido: .\n"
     ]
    }
   ],
   "source": [
    "nlp = Spanish()\n",
    "\n",
    "# Define la función getter que toma un token y devuelve su texto al revés\n",
    "def get_reversed(token):\n",
    "    return token.text[::-1]\n",
    "\n",
    "\n",
    "# Registra la extensión de propiedad del Token, \"reversed\", con\n",
    "# el getter get_reversed\n",
    "Token.set_extension(\"reversed\", getter=get_reversed, force=True)\n",
    "\n",
    "# Procesa el texto e imprime en pantalla el atributo \"reversed\"\n",
    "# para cada token\n",
    "doc = nlp(\"Todas las generalizaciones son falsas, incluyendo esta.\")\n",
    "for token in doc:\n",
    "    print(\"invertido:\", token._.reversed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capítulo 4: Entrenando un modelo de red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "classification.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
